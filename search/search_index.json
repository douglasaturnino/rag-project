{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipeline RAG completa: Ingest\u00e3o, Consulta e Obserabilidade","text":"<p>Este projeto foi feito acompanhando a live da jornada de dados</p> <p>Este projeto demonstra como construir um pipeline completo de Recupera\u00e7\u00e3o e Gera\u00e7\u00e3o (RAG) usando: - LangChain \u2192 integra\u00e7\u00e3o entre LLMs e bancos vetoriais. - LangGraph \u2192 orquestra\u00e7\u00e3o de n\u00f3s e controle de fluxo. - Langfuse \u2192 observabilidade e logging de cada execu\u00e7\u00e3o. - Qdrant \u2192 vetor store local (armazenamento vetorial). - Streamlit \u2192 interface interativa para o usu\u00e1rio final.</p> <p>O objetivo \u00e9 mostrar, de forma pr\u00e1tica, como montar uma aplica\u00e7\u00e3o jur\u00eddica capaz de responder perguntas sobre s\u00famulas do Tribunal de Contas de Minas Gerais (TCEMG) com base em documentos PDF indexados.</p>"},{"location":"#estrutura-do-projeto","title":"\u2699\ufe0f Estrutura do Projeto","text":"<pre><code>rag-project/\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 graph\n\u2502   \u2502   \u251c\u2500\u2500 prompt.py         # Prompts e templates do LLM\n\u2502   \u2502   \u2514\u2500\u2500 rag_graph.py      # Grafo principal do LangGraph \n\u2502   \u251c\u2500\u2500 ingest\n\u2502   \u2502   \u251c\u2500\u2500 embed_qdrant.py   # Embeddings e conex\u00e3o com Qdrant\n\u2502   \u2502   \u251c\u2500\u2500 extract_text.py   # Extra\u00e7\u00e3o de texto e metadados dos PDFs\n\u2502   \u251c\u2500\u2500 retrieval\n\u2502   \u2502   \u251c\u2500\u2500 retriever.py      # SelfQueryRetriever + Qdrant\n\u2502   \u2502   \u2514\u2500\u2500 self_query.py     # Defini\u00e7\u00e3o dos metadados e filtros\n\u2502   \u2514\u2500\u2500 utils\n\u2502       \u2514\u2500\u2500 settings.py       # Configura\u00e7\u00f5es globais do app\n\u251c\u2500\u2500 app.py                    # Interface Streamlit\n\u251c\u2500\u2500 injest_text.py            # Arquivo inicial para processar os PDFs\n\u251c\u2500\u2500 pyproject.toml            \n\u251c\u2500\u2500 README.md\n</code></pre>"},{"location":"#conceitos-chave","title":"\ud83e\udde0 Conceitos-Chave","text":"<ul> <li> <p>RAG (Retrieval-Augmented Generation)   Combina busca sem\u00e2ntica com gera\u00e7\u00e3o de texto. O modelo recupera contexto real antes de gerar a resposta.  </p> </li> <li> <p>Self-Query Retriever   Permite que o pr\u00f3prio modelo entenda a pergunta e monte filtros estruturados automaticamente (por exemplo, filtrar por <code>status_atual</code> ou <code>data_status</code>).  </p> </li> <li> <p>LangGraph   Controla o fluxo entre os n\u00f3s (<code>retrieve \u2192 generate \u2192 END</code>) e o estado global.  </p> </li> <li> <p>Langfuse   Garante observabilidade completa: rastreamento de spans, tokens e m\u00e9tricas de cada execu\u00e7\u00e3o.</p> </li> </ul>"},{"location":"#como-executar-o-projeto-localmente","title":"\ud83d\ude80 Como Executar o Projeto Localmente","text":""},{"location":"#1-pre-requisitos","title":"1\ufe0f\u20e3 Pr\u00e9-requisitos","text":"<ul> <li>Docker Desktop </li> <li>Python 3.11+  </li> <li>Conta gratuita em:</li> <li>OpenAI Platform</li> <li>Langfuse Cloud</li> <li>Clonar o reposit\u00f3rio:</li> </ul> <pre><code>git clone https://github.com/douglasaturnino/rag-project.git\ncd rag-project\n</code></pre>"},{"location":"#2-subir-o-qdrant-localmente","title":"2\ufe0f\u20e3 Subir o Qdrant localmente","text":"<pre><code>docker run -d --name qdrant -p 6333:6333 qdrant/qdrant\n</code></pre>"},{"location":"#3-configurar-variaveis-de-ambiente","title":"3\ufe0f\u20e3 Configurar vari\u00e1veis de ambiente","text":"<p>Renomeie o arquivo <code>env.example</code> para <code>.env</code> na raiz do projeto:</p> <pre><code>TEMPERATURE=0\nGOOGLE_API_KEY=\"&lt;GOOGLE_API_KEY&gt;\"\n# OPENAI_API_KEY=\"&lt;OPENAI_API_KEY&gt;\"\nMODEL_NAME=google_genai:gemini-2.5-flash      # \"openai:o3-mini\"\nLANGFUSE_PUBLIC_KEY=\"&lt;LANGFUSE_PUBLIC_KEY&gt;\"\nLANGFUSE_SECRET_KEY=\"&lt;LANGFUSE_SECRET_KEY&gt;\"\nLANGFUSE_HOST=https://us.cloud.langfuse.com\nQDRANT_HOST=localhost\nQDRANT_PORT=6333\n</code></pre>"},{"location":"#4-instalar-dependencias","title":"4\ufe0f\u20e3 Instalar depend\u00eancias","text":""},{"location":"#para-instalar-utilizando-o-pip","title":"Para instalar utilizando o pip","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate     # (ou .venv\\Scripts\\activate no Windows)\npip install -r requirements.txt\n</code></pre>"},{"location":"#para-instalar-utilizando-o-uv","title":"Para instalar utilizando o uv","text":"<pre><code>uv sync  \nsource .venv/bin/activate     # (ou .venv\\Scripts\\activate no Windows)\n</code></pre>"},{"location":"#5-ingerir-os-documentos","title":"5\ufe0f\u20e3 Ingerir os documentos","text":"<p>Coloque seus arquivos PDF na pasta <code>sumulas/</code>. Voc\u00ea pode baixar documentos oficiais do TCE-MG aqui: \ud83d\udd17 https://www.tce.mg.gov.br/Noticia/Detalhe/67</p> <p>Depois execute:</p> <pre><code>python injest_text.py\n</code></pre> <p>Isso criar\u00e1 a cole\u00e7\u00e3o <code>sumulas_jornada</code> no Qdrant, extraindo texto e metadados automaticamente.</p>"},{"location":"#6-executar-o-app","title":"6\ufe0f\u20e3 Executar o app","text":"<pre><code>streamlit run app.py\n</code></pre> <p>Abra o navegador em http://localhost:8501.</p>"},{"location":"#como-o-projeto-lida-com-datas","title":"\ud83e\uddee Como o Projeto Lida com Datas","text":"<p>\ud83d\uddd3\ufe0f O Qdrant s\u00f3 suporta compara\u00e7\u00f5es (<code>&lt;</code>, <code>&gt;</code>) em valores num\u00e9ricos, n\u00e3o em strings.</p> <p>Como os metadados vinham no formato <code>DD/MM/AA</code>, as compara\u00e7\u00f5es lexicogr\u00e1ficas n\u00e3o funcionam: <pre><code>'01/01/10' &gt; '31/12/09'  # (errado, pois compara texto)\n</code></pre></p> <p>Solu\u00e7\u00e3o: Durante a ingest\u00e3o, o campo <code>data_status</code> \u00e9 convertido para ano (inteiro), ex.:</p> <pre><code>\"07/04/14\" \u2192 2014\n</code></pre> <p>Isso permite aplicar filtros num\u00e9ricos no SelfQueryRetriever:</p> <pre><code>AttributeInfo(\n    name=\"data_status\",\n    description=\"Ano da publica\u00e7\u00e3o (inteiro). Ex.: 2014. Use lt/lte/gt/gte para comparar anos.\",\n    type=\"integer\",\n)\n</code></pre>"},{"location":"#dica-sobre-datas-e-comparacoes","title":"\ud83d\udca1 Dica sobre Datas e Compara\u00e7\u00f5es","text":"<p>O Qdrant Translator geralmente s\u00f3 permite filtros de igualdade (==). Filtros de compara\u00e7\u00e3o (<code>&lt;</code>, <code>&gt;</code>) em strings n\u00e3o funcionam para datas, a menos que o formato seja ISO 8601 (YYYY-MM-DD). Como este projeto usa DD/MM/AA, \u00e9 essencial armazenar o campo <code>data_status</code> como inteiro (ano).</p>"},{"location":"#observabilidade-com-langfuse","title":"\ud83d\udcca Observabilidade com Langfuse","text":"<p>O projeto utiliza a cloud langfuse para a observabilidade ent\u00e3o ser\u00e1 necessario fazer o cadastro na plataforma  Cada execu\u00e7\u00e3o \u00e9 rastreada no painel da Langfuse, incluindo:</p> <ul> <li>Prompt e contexto usados  </li> <li>Tokens consumidos  </li> <li>Tempo de resposta  </li> <li>Cadeia de n\u00f3s (<code>retrieve \u2192 generate</code>)  </li> </ul> <p>\ud83d\udd17 Acesse o painel Langfuse Cloud para visualizar seus traces em tempo real.</p>"},{"location":"#referencias-tecnicas","title":"\ud83d\udd17 Refer\u00eancias T\u00e9cnicas","text":"<ul> <li>\ud83d\udcc4 MarkItDown (Microsoft) \u2013 extra\u00e7\u00e3o de texto e metadados dos PDFs.  </li> <li>\ud83d\udcbe Qdrant LangChain API \u2013 integra\u00e7\u00e3o vetorial.  </li> <li>\ud83d\udd0d SelfQueryRetriever Docs </li> <li>\u2699\ufe0f Qdrant Quickstart </li> <li>\ud83d\udcda Jornada de Dados</li> </ul>"},{"location":"graph/prompt/","title":"prompt","text":""},{"location":"graph/prompt/#app.graph.prompt","title":"<code>app.graph.prompt</code>","text":"SYSTEM_PROMPT_JURIDICO <p>Este \u00e9 um prompt de sistema que define a estrutura e as diretrizes para o modelo de IA responder perguntas jur\u00eddicas.</p> <p>Argumento:   question: A pergunta do usu\u00e1rio que deve ser respondida.</p> <p>context: O conjunto de trechos de documentos (s\u00famulas)     fornecidos para responder \u00e0 pergunta.</p> PROMPT_EXTRACT <p>Este prompt \u00e9 usado para extrair metadados e dividir um documento em chunks espec\u00edficos.</p> <p>Argumento   pdf_name: Nome do arquivo PDF que ser\u00e1 processado.</p> <p>text_content: O texto da s\u00famula que ser\u00e1 analisado.</p>"},{"location":"graph/rag_graph/","title":"rag_graph","text":""},{"location":"graph/rag_graph/#app.graph.rag_graph","title":"<code>app.graph.rag_graph</code>","text":""},{"location":"graph/rag_graph/#app.graph.rag_graph.RAGState","title":"<code>RAGState</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Representa o estado do Grafo de Recupera\u00e7\u00e3o e Gera\u00e7\u00e3o (RAG) durante o fluxo de execu\u00e7\u00e3o.</p> <p>Esta classe mant\u00e9m as informa\u00e7\u00f5es necess\u00e1rias para realizar a consulta, recuperar documentos, gerar respostas e manter o hist\u00f3rico de mensagens.</p> <p>Attributes:</p> Name Type Description <code>question</code> <code>str</code> <p>A pergunta realizada pelo usu\u00e1rio, que ser\u00e1 usada para gerar a consulta e recuperar documentos.</p> <code>docs</code> <code>List[Document]</code> <p>A lista de documentos recuperados que fornecem contexto para a gera\u00e7\u00e3o da resposta.</p> <code>answer</code> <code>Generator[str, None, None]</code> <p>Um gerador de strings que emite os tokens da resposta gerada em formato de stream.</p> <code>generated_query</code> <code>str</code> <p>A consulta gerada com base na pergunta original.</p> <code>generated_filter</code> <code>str</code> <p>O filtro gerado para a consulta, formatado para exibi\u00e7\u00e3o amig\u00e1vel.</p> <code>messages</code> <code>list</code> <p>Lista de mensagens (geralmente trocas entre o usu\u00e1rio e o sistema) associadas ao estado atual do fluxo.</p> Source code in <code>app/graph/rag_graph.py</code> <pre><code>class RAGState(TypedDict):\n    \"\"\"\n    Representa o estado do Grafo de Recupera\u00e7\u00e3o e Gera\u00e7\u00e3o (RAG) durante o fluxo de execu\u00e7\u00e3o.\n\n    Esta classe mant\u00e9m as informa\u00e7\u00f5es necess\u00e1rias para realizar a consulta, recuperar documentos, gerar respostas\n    e manter o hist\u00f3rico de mensagens.\n\n    Attributes:\n        question (str): A pergunta realizada pelo usu\u00e1rio, que ser\u00e1 usada para gerar a consulta e recuperar documentos.\n        docs (List[Document]): A lista de documentos recuperados que fornecem contexto para a gera\u00e7\u00e3o da resposta.\n        answer (Generator[str, None, None]): Um gerador de strings que emite os tokens da resposta gerada em formato de stream.\n        generated_query (str): A consulta gerada com base na pergunta original.\n        generated_filter (str): O filtro gerado para a consulta, formatado para exibi\u00e7\u00e3o amig\u00e1vel.\n        messages (list): Lista de mensagens (geralmente trocas entre o usu\u00e1rio e o sistema) associadas ao estado atual do fluxo.\n    \"\"\"\n\n    question: str\n    docs: List[Document]\n    answer: Generator[str, None, None]\n    generated_query: str\n    generated_filter: str\n    messages: Annotated[list, add_messages]\n</code></pre>"},{"location":"graph/rag_graph/#app.graph.rag_graph.build_streaming_graph","title":"<code>build_streaming_graph(collection_name='sumulas_jornada', k=5)</code>","text":"<p>Compila o grafo LangGraph com os n\u00f3s para streaming, incluindo os n\u00f3s de recupera\u00e7\u00e3o e gera\u00e7\u00e3o.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>(str, opcional)</code> <p>Nome da cole\u00e7\u00e3o de dados a ser consultada. Padr\u00e3o \u00e9 \"sumulas_jornada\".</p> <code>'sumulas_jornada'</code> <code>k</code> <code>(int, opcional)</code> <p>N\u00famero de resultados a serem retornados pela consulta. Padr\u00e3o \u00e9 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>StateGraph</code> <p>O grafo compilado com os n\u00f3s configurados.</p> Source code in <code>app/graph/rag_graph.py</code> <pre><code>def build_streaming_graph(\n    collection_name: str = \"sumulas_jornada\", k: int = 5\n) -&gt; StateGraph:\n    \"\"\"\n    Compila o grafo LangGraph com os n\u00f3s para streaming, incluindo os n\u00f3s de recupera\u00e7\u00e3o e gera\u00e7\u00e3o.\n\n    Args:\n        collection_name (str, opcional): Nome da cole\u00e7\u00e3o de dados a ser consultada. Padr\u00e3o \u00e9 \"sumulas_jornada\".\n        k (int, opcional): N\u00famero de resultados a serem retornados pela consulta. Padr\u00e3o \u00e9 5.\n\n    Returns:\n        (StateGraph): O grafo compilado com os n\u00f3s configurados.\n    \"\"\"\n    graph = StateGraph(RAGState)\n    graph.add_node(\n        \"retrieve\",\n        lambda s, config: retrieve(\n            s, config=config, collection_name=collection_name, k=k\n        ),\n    )\n    graph.add_node(\"generate\", generate_stream)\n    graph.set_entry_point(\"retrieve\")\n    graph.add_edge(\"retrieve\", \"generate\")\n    graph.add_edge(\"generate\", END)\n    return graph.compile()\n</code></pre>"},{"location":"graph/rag_graph/#app.graph.rag_graph.generate_stream","title":"<code>generate_stream(state, config)</code>","text":"<p>Gera a resposta final em formato de stream, utilizando o modelo de linguagem e o prompt definidos.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>RAGState</code> <p>O estado atual do grafo, incluindo a pergunta e documentos.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configura\u00e7\u00e3o para execu\u00e7\u00e3o do grafo.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Um dicion\u00e1rio contendo o fluxo de resposta gerado.</p> Source code in <code>app/graph/rag_graph.py</code> <pre><code>def generate_stream(state: RAGState, config: RunnableConfig) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gera a resposta final em formato de stream, utilizando o modelo de linguagem e o prompt definidos.\n\n    Args:\n        state (RAGState): O estado atual do grafo, incluindo a pergunta e documentos.\n        config (RunnableConfig): Configura\u00e7\u00e3o para execu\u00e7\u00e3o do grafo.\n\n    Returns:\n        (Dict[str, Any]): Um dicion\u00e1rio contendo o fluxo de resposta gerado.\n    \"\"\"\n    print(\"Executando o n\u00f3 de gera\u00e7\u00e3o...\")\n    QA_PROMPT = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", SYSTEM_PROMPT_JURIDICO),\n            (\n                \"human\",\n                \"Pergunta: {question}\\n\\nContexto (trechos):\\n{context}\\n\\nResponda de forma direta. Ao final, liste fontes no formato: (Status da S\u00famula: metadata.status_atual, N\u00famero da S\u00famula: metadata.num_sumula, Data da Publica\u00e7\u00e3o:  metadata.data_status).\",\n            ),\n        ]\n    )\n\n    embedder = EmbeddingSelfQuery()\n    llm = embedder.llm\n    context = _format_docs(state.get(\"docs\", []))\n    chain = QA_PROMPT | llm | StrOutputParser()\n\n    answer_stream = chain.stream(\n        {\"question\": state[\"question\"], \"context\": context},\n        config=config,\n    )\n    return {\"answer\": answer_stream}\n</code></pre>"},{"location":"graph/rag_graph/#app.graph.rag_graph.retrieve","title":"<code>retrieve(state, config, collection_name='sumulas_jornada', k=10)</code>","text":"<p>Executa o SelfQueryRetriever e extrai os detalhes da consulta gerada.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>RAGState</code> <p>O estado atual do grafo, incluindo a pergunta e documentos.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configura\u00e7\u00e3o para execu\u00e7\u00e3o do grafo.</p> required <code>collection_name</code> <code>(str, opcional)</code> <p>Nome da cole\u00e7\u00e3o de dados a ser consultada. Padr\u00e3o \u00e9 \"sumulas_jornada\".</p> <code>'sumulas_jornada'</code> <code>k</code> <code>(int, opcional)</code> <p>N\u00famero de resultados a serem retornados pela consulta. Padr\u00e3o \u00e9 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Um dicion\u00e1rio contendo os documentos recuperados, a consulta gerada e o filtro formatado.</p> Source code in <code>app/graph/rag_graph.py</code> <pre><code>def retrieve(\n    state: RAGState,\n    config: RunnableConfig,\n    collection_name: str = \"sumulas_jornada\",\n    k: int = 10,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Executa o SelfQueryRetriever e extrai os detalhes da consulta gerada.\n\n    Args:\n        state (RAGState): O estado atual do grafo, incluindo a pergunta e documentos.\n        config (RunnableConfig): Configura\u00e7\u00e3o para execu\u00e7\u00e3o do grafo.\n        collection_name (str, opcional): Nome da cole\u00e7\u00e3o de dados a ser consultada. Padr\u00e3o \u00e9 \"sumulas_jornada\".\n        k (int, opcional): N\u00famero de resultados a serem retornados pela consulta. Padr\u00e3o \u00e9 10.\n\n    Returns:\n        (Dict[str, Any]): Um dicion\u00e1rio contendo os documentos recuperados, a consulta gerada e o filtro formatado.\n    \"\"\"\n    print(\"Executando o n\u00f3 de recupera\u00e7\u00e3o...\")\n    cfg = SelfQueryConfig(collection_name=collection_name, k=k)\n    retriever = build_self_query_retriever(cfg)\n\n    structured_query: StructuredQuery = retriever.query_constructor.invoke(\n        {\"query\": state[\"question\"]}, config=config\n    )\n    docs = retriever.invoke(state[\"question\"], config=config)\n\n    print(f\"Busca finalizada. Encontrados {len(docs)} documentos.\")\n    return {\n        \"docs\": docs,\n        \"generated_query\": structured_query.query,\n        \"generated_filter\": _format_filter_for_display(\n            structured_query.filter\n        ),\n    }\n</code></pre>"},{"location":"graph/rag_graph/#app.graph.rag_graph.run_streaming_rag","title":"<code>run_streaming_rag(question)</code>","text":"<p>Fun\u00e7\u00e3o de alto n\u00edvel que executa o fluxo RAG e retorna um gerador de eventos para o frontend.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>A pergunta a ser processada pelo grafo.</p> required <p>Returns:</p> Type Description <code>Generator[Dict[str, Any], None, None]</code> <p>Um gerador que emite eventos em formato de dicion\u00e1rio durante o fluxo.</p> Source code in <code>app/graph/rag_graph.py</code> <pre><code>def run_streaming_rag(question: str) -&gt; Generator[Dict[str, Any], None, None]:\n    \"\"\"\n    Fun\u00e7\u00e3o de alto n\u00edvel que executa o fluxo RAG e retorna um gerador de eventos para o frontend.\n\n    Args:\n        question (str): A pergunta a ser processada pelo grafo.\n\n    Returns:\n        (Generator[Dict[str, Any], None, None]): Um gerador que emite eventos em formato de dicion\u00e1rio durante o fluxo.\n    \"\"\"\n\n    # run_config = {\"callbacks\": [langfuse_handler], \"run_name\": \"Chat\"}\n    run_config = RunnableConfig(\n        callbacks=[langfuse_handler],\n        run_name=\"Chat\",\n        tags=[\"live-demo\", \"sumulas\"],\n        metadata={\"collection\": \"sumulas_jornada\", \"k\": 10, \"user\": \"Douglas\"},\n    )\n\n    initial_state: RAGState = {\"question\": question, \"messages\": []}\n    final_state = {}\n\n    # Executa o grafo em modo streaming\n    for event in COMPILED_GRAPH.stream(initial_state, config=run_config):\n        if \"retrieve\" in event:\n            output = event[\"retrieve\"]\n            yield {\n                \"type\": \"details\",\n                \"data\": {\n                    \"query\": output[\"generated_query\"],\n                    \"filter\": output[\"generated_filter\"],\n                },\n            }\n\n        if \"generate\" in event:\n            answer_stream = event[\"generate\"][\"answer\"]\n            # Itera sobre o gerador de tokens da resposta\n            for token in answer_stream:\n                yield {\"type\": \"token\", \"data\": token}\n\n        if END in event:\n            final_state = event[END]\n\n    # Formata e retorna as fontes no final do fluxo\n    docs = final_state.get(\"docs\", [])\n    sources = [\n        {\n            \"pdf_name\": d.metadata.get(\"pdf_name\"),\n            \"data_status\": d.metadata.get(\"data_status\"),\n            \"data_status_ano\": d.metadata.get(\"data_status_ano\"),\n            \"status_atual\": d.metadata.get(\"status_atual\"),\n            \"num_sumula\": d.metadata.get(\"num_sumula\"),\n            \"chunk_type\": d.metadata.get(\"chunk_type\"),\n        }\n        for d in docs\n    ]\n    yield {\"type\": \"sources\", \"data\": sources}\n</code></pre>"},{"location":"ingest/embed_qdrant/","title":"embed_qdrant","text":""},{"location":"ingest/embed_qdrant/#app.ingest.embed_qdrant","title":"<code>app.ingest.embed_qdrant</code>","text":""},{"location":"ingest/embed_qdrant/#app.ingest.embed_qdrant.EmbeddingSelfQuery","title":"<code>EmbeddingSelfQuery</code>","text":"<p>Classe que gerencia a inicializa\u00e7\u00e3o do modelo de linguagem, a configura\u00e7\u00e3o do cliente Qdrant e o modelo de embeddings para realizar consultas.</p> <p>A classe fornece m\u00e9todos para obter um <code>QdrantVectorStore</code>, que \u00e9 usado para armazenar e recuperar embeddings de vetores para a realiza\u00e7\u00e3o de consultas baseadas em sem\u00e2ntica.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseChatModel</code> <p>O modelo de linguagem inicializado para gerar respostas baseadas em consultas.</p> <code>client</code> <code>QdrantClient</code> <p>O cliente Qdrant usado para interagir com o banco de dados Qdrant.</p> <code>model</code> <code>FastEmbedEmbeddings</code> <p>O modelo de embeddings que converte texto em representa\u00e7\u00f5es vetoriais.</p> Source code in <code>app/ingest/embed_qdrant.py</code> <pre><code>class EmbeddingSelfQuery:\n    \"\"\"\n    Classe que gerencia a inicializa\u00e7\u00e3o do modelo de linguagem, a configura\u00e7\u00e3o do cliente Qdrant e o modelo de embeddings para realizar consultas.\n\n    A classe fornece m\u00e9todos para obter um `QdrantVectorStore`, que \u00e9 usado para armazenar e recuperar embeddings de vetores\n    para a realiza\u00e7\u00e3o de consultas baseadas em sem\u00e2ntica.\n\n    Attributes:\n        llm (BaseChatModel): O modelo de linguagem inicializado para gerar respostas baseadas em consultas.\n        client (QdrantClient): O cliente Qdrant usado para interagir com o banco de dados Qdrant.\n        model (FastEmbedEmbeddings): O modelo de embeddings que converte texto em representa\u00e7\u00f5es vetoriais.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Inicializa o modelo de linguagem, o cliente Qdrant e o modelo de embeddings.\n\n        O modelo de linguagem \u00e9 inicializado com configura\u00e7\u00f5es definidas nas vari\u00e1veis de ambiente.\n        O cliente Qdrant \u00e9 configurado com os par\u00e2metros de host e porta definidos nas configura\u00e7\u00f5es.\n        O modelo de embeddings \u00e9 inicializado para convers\u00f5es de texto em vetores de alta qualidade.\n        \"\"\"\n\n        self.llm = init_chat_model(\n            model=settings.MODEL_NAME,\n            temperature=settings.TEMPERATURE,\n        )\n        self.client = QdrantClient(\n            host=settings.QDRANT_HOST,\n            port=settings.QDRANT_PORT,\n            timeout=120,\n        )\n\n        self.model = FastEmbedEmbeddings(model_name=settings.EMBEDDINGS_NAME)\n\n    def get_qdrant_vector_store(\n        self, collection_name: str\n    ) -&gt; QdrantVectorStore:\n        \"\"\"\n        Retorna uma inst\u00e2ncia do `QdrantVectorStore` configurada com o cliente Qdrant,\n        o modelo de embeddings e os par\u00e2metros da cole\u00e7\u00e3o.\n\n        Args:\n            collection_name (str): Nome da cole\u00e7\u00e3o onde os vetores ser\u00e3o armazenados no Qdrant.\n\n        Returns:\n            (QdrantVectorStore): A inst\u00e2ncia configurada do `QdrantVectorStore`.\n        \"\"\"\n        return QdrantVectorStore(\n            client=self.client,\n            collection_name=collection_name,\n            embedding=self.model,\n            sparse_vector_name=\"text-sparse\",\n            vector_name=\"text-dense\",\n        )\n</code></pre>"},{"location":"ingest/embed_qdrant/#app.ingest.embed_qdrant.EmbeddingSelfQuery.__init__","title":"<code>__init__()</code>","text":"<p>Inicializa o modelo de linguagem, o cliente Qdrant e o modelo de embeddings.</p> <p>O modelo de linguagem \u00e9 inicializado com configura\u00e7\u00f5es definidas nas vari\u00e1veis de ambiente. O cliente Qdrant \u00e9 configurado com os par\u00e2metros de host e porta definidos nas configura\u00e7\u00f5es. O modelo de embeddings \u00e9 inicializado para convers\u00f5es de texto em vetores de alta qualidade.</p> Source code in <code>app/ingest/embed_qdrant.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Inicializa o modelo de linguagem, o cliente Qdrant e o modelo de embeddings.\n\n    O modelo de linguagem \u00e9 inicializado com configura\u00e7\u00f5es definidas nas vari\u00e1veis de ambiente.\n    O cliente Qdrant \u00e9 configurado com os par\u00e2metros de host e porta definidos nas configura\u00e7\u00f5es.\n    O modelo de embeddings \u00e9 inicializado para convers\u00f5es de texto em vetores de alta qualidade.\n    \"\"\"\n\n    self.llm = init_chat_model(\n        model=settings.MODEL_NAME,\n        temperature=settings.TEMPERATURE,\n    )\n    self.client = QdrantClient(\n        host=settings.QDRANT_HOST,\n        port=settings.QDRANT_PORT,\n        timeout=120,\n    )\n\n    self.model = FastEmbedEmbeddings(model_name=settings.EMBEDDINGS_NAME)\n</code></pre>"},{"location":"ingest/embed_qdrant/#app.ingest.embed_qdrant.EmbeddingSelfQuery.get_qdrant_vector_store","title":"<code>get_qdrant_vector_store(collection_name)</code>","text":"<p>Retorna uma inst\u00e2ncia do <code>QdrantVectorStore</code> configurada com o cliente Qdrant, o modelo de embeddings e os par\u00e2metros da cole\u00e7\u00e3o.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Nome da cole\u00e7\u00e3o onde os vetores ser\u00e3o armazenados no Qdrant.</p> required <p>Returns:</p> Type Description <code>QdrantVectorStore</code> <p>A inst\u00e2ncia configurada do <code>QdrantVectorStore</code>.</p> Source code in <code>app/ingest/embed_qdrant.py</code> <pre><code>def get_qdrant_vector_store(\n    self, collection_name: str\n) -&gt; QdrantVectorStore:\n    \"\"\"\n    Retorna uma inst\u00e2ncia do `QdrantVectorStore` configurada com o cliente Qdrant,\n    o modelo de embeddings e os par\u00e2metros da cole\u00e7\u00e3o.\n\n    Args:\n        collection_name (str): Nome da cole\u00e7\u00e3o onde os vetores ser\u00e3o armazenados no Qdrant.\n\n    Returns:\n        (QdrantVectorStore): A inst\u00e2ncia configurada do `QdrantVectorStore`.\n    \"\"\"\n    return QdrantVectorStore(\n        client=self.client,\n        collection_name=collection_name,\n        embedding=self.model,\n        sparse_vector_name=\"text-sparse\",\n        vector_name=\"text-dense\",\n    )\n</code></pre>"},{"location":"ingest/extract_text/","title":"extract_text","text":""},{"location":"ingest/extract_text/#app.ingest.extract_text","title":"<code>app.ingest.extract_text</code>","text":""},{"location":"ingest/extract_text/#app.ingest.extract_text.create_collection_if_not_exists","title":"<code>create_collection_if_not_exists(embedder, collection)</code>","text":"<p>Cria uma cole\u00e7\u00e3o no Qdrant se ela n\u00e3o existir, configurando os par\u00e2metros para vetores densos e esparsos.</p> <p>A fun\u00e7\u00e3o verifica se a cole\u00e7\u00e3o especificada j\u00e1 existe no Qdrant. Se n\u00e3o existir, cria a cole\u00e7\u00e3o com as configura\u00e7\u00f5es adequadas para vetores densos e esparsos, usando o modelo de embeddings configurado no <code>embedder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>embedder</code> <code>EmbeddingSelfQuery</code> <p>O objeto que cont\u00e9m o cliente Qdrant e o modelo de embeddings.</p> required <code>collection</code> <code>str</code> <p>O nome da cole\u00e7\u00e3o a ser criada ou verificada.</p> required Source code in <code>app/ingest/extract_text.py</code> <pre><code>def create_collection_if_not_exists(\n    embedder: EmbeddingSelfQuery, collection: str\n) -&gt; None:\n    \"\"\"\n    Cria uma cole\u00e7\u00e3o no Qdrant se ela n\u00e3o existir, configurando os par\u00e2metros para vetores densos e esparsos.\n\n    A fun\u00e7\u00e3o verifica se a cole\u00e7\u00e3o especificada j\u00e1 existe no Qdrant. Se n\u00e3o existir, cria a cole\u00e7\u00e3o com as configura\u00e7\u00f5es adequadas\n    para vetores densos e esparsos, usando o modelo de embeddings configurado no `embedder`.\n\n    Args:\n        embedder (EmbeddingSelfQuery): O objeto que cont\u00e9m o cliente Qdrant e o modelo de embeddings.\n        collection (str): O nome da cole\u00e7\u00e3o a ser criada ou verificada.\n    \"\"\"\n\n    # Cria cole\u00e7\u00e3o se n\u00e3o existir\n\n    if embedder.client.collection_exists(collection_name=collection):\n        logger.info(f\"Cole\u00e7\u00e3o '{collection}' j\u00e1 existe.\")\n        return\n\n    embedder.client.create_collection(\n        collection_name=collection,\n        vectors_config={\n            \"text-dense\": VectorParams(\n                size=embedder.model.model.embedding_size,\n                distance=Distance.COSINE,\n            )\n        },\n        sparse_vectors_config={\n            \"text-sparse\": SparseVectorParams()  # sem size para esparso\n        },\n    )\n    logger.info(f\"Cole\u00e7\u00e3o '{collection}' criada.\")\n</code></pre>"},{"location":"ingest/extract_text/#app.ingest.extract_text.main","title":"<code>main(collection='sumulas_jornada', pasta_pdfs='sumulas')</code>","text":"<p>Fun\u00e7\u00e3o principal que orquestra o processamento de arquivos PDF, a cria\u00e7\u00e3o de cole\u00e7\u00f5es no Qdrant e a adi\u00e7\u00e3o de vetores.</p> <p>Esta fun\u00e7\u00e3o: 1. Verifica e cria a cole\u00e7\u00e3o no Qdrant se n\u00e3o existir. 2. Processa os arquivos PDF da pasta especificada, extraindo os chunks e metadados. 3. Adiciona os textos e metadados ao Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>(str, opcional)</code> <p>Nome da cole\u00e7\u00e3o do Qdrant. Padr\u00e3o \u00e9 \"sumulas_jornada\".</p> <code>'sumulas_jornada'</code> <code>pasta_pdfs</code> <code>(str, opcional)</code> <p>Caminho da pasta contendo os arquivos PDF a serem processados. Padr\u00e3o \u00e9 \"sumulas\".</p> <code>'sumulas'</code> Source code in <code>app/ingest/extract_text.py</code> <pre><code>def main(\n    collection: str = \"sumulas_jornada\",\n    pasta_pdfs: str = \"sumulas\",\n) -&gt; None:\n    \"\"\"\n    Fun\u00e7\u00e3o principal que orquestra o processamento de arquivos PDF, a cria\u00e7\u00e3o de cole\u00e7\u00f5es no Qdrant e a adi\u00e7\u00e3o de vetores.\n\n    Esta fun\u00e7\u00e3o:\n    1. Verifica e cria a cole\u00e7\u00e3o no Qdrant se n\u00e3o existir.\n    2. Processa os arquivos PDF da pasta especificada, extraindo os chunks e metadados.\n    3. Adiciona os textos e metadados ao Qdrant.\n\n    Args:\n        collection (str, opcional): Nome da cole\u00e7\u00e3o do Qdrant. Padr\u00e3o \u00e9 \"sumulas_jornada\".\n        pasta_pdfs (str, opcional): Caminho da pasta contendo os arquivos PDF a serem processados. Padr\u00e3o \u00e9 \"sumulas\".\n\n    \"\"\"\n\n    embedder = EmbeddingSelfQuery()\n\n    create_collection_if_not_exists(embedder, collection)\n\n    vector_store = embedder.get_qdrant_vector_store(collection)\n    pdf_files = list(Path(pasta_pdfs).glob(\"*.pdf\"))\n    if not pdf_files:\n        logger.info(\"Nenhum PDF encontrado na pasta.\")\n        return\n\n    total_chunks = 0\n    for pdf_file in pdf_files:\n        logger.debug(f\"Processando {pdf_file.name}.\")\n        chunks = process_pdf_file(str(pdf_file), embedder)\n        if not chunks:\n            continue\n        texts = [c[\"text\"] for c in chunks]\n        metadatas = [c[\"metadata\"] for c in chunks]\n        vector_store.add_texts(texts=texts, metadatas=metadatas)\n        total_chunks += len(chunks)\n        logger.debug(f\"{pdf_file.name} processada.\")\n\n    logger.success(\n        f\"\u2705 {len(pdf_files)} PDFs processados. {total_chunks} chunks inseridos no Qdrant.\"\n    )\n</code></pre>"},{"location":"ingest/extract_text/#app.ingest.extract_text.process_pdf_file","title":"<code>process_pdf_file(file_path, embedder)</code>","text":"<p>Processa um arquivo PDF, extraindo metadados e dividindo o conte\u00fado em at\u00e9 3 chunks, retornando uma lista de dicion\u00e1rios.</p> <p>A fun\u00e7\u00e3o usa o modelo de linguagem do <code>embedder</code> para extrair os metadados da s\u00famula e dividir o texto em at\u00e9 tr\u00eas partes principais:</p> <ol> <li> <p>conteudo_principal: O conte\u00fado principal do documento at\u00e9 a se\u00e7\u00e3o de \"REFER\u00caNCIAS NORMATIVAS\".</p> </li> <li> <p>referencias_normativas: O conte\u00fado entre \"REFER\u00caNCIAS NORMATIVAS:\" e \"PRECEDENTES:\".</p> </li> <li> <p>precedentes: O conte\u00fado ap\u00f3s a se\u00e7\u00e3o de \"PRECEDENTES:\".</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Caminho para o arquivo PDF que ser\u00e1 processado.</p> required <code>embedder</code> <code>EmbeddingSelfQuery</code> <p>O objeto que cont\u00e9m o modelo de linguagem usado para extrair os metadados e chunks.</p> required <p>Returns:</p> Name Type Description <code>processed</code> <code>List[Dict[str, Any]]</code> <p>Uma lista de dicion\u00e1rios contendo os textos dos chunks e seus respectivos metadados. Cada dicion\u00e1rio tem a estrutura: <pre><code>{\n    \"text\": &lt;texto do chunk&gt;,\n    \"metadata\": {\n        \"num_sumula\": &lt;n\u00famero da s\u00famula&gt;,\n        \"data_status\": &lt;data de status&gt;,\n        \"data_status_ano\": &lt;ano da data de status&gt;,\n        \"status_atual\": &lt;status atual&gt;,\n        \"pdf_name\": &lt;nome do arquivo PDF&gt;,\n        \"chunk_type\": &lt;tipo do chunk&gt;,\n        \"chunk_index\": &lt;\u00edndice do chunk&gt;\n    }\n}\n</code></pre></p> Source code in <code>app/ingest/extract_text.py</code> <pre><code>def process_pdf_file(\n    file_path: str, embedder: EmbeddingSelfQuery\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Processa um arquivo PDF, extraindo metadados e dividindo o conte\u00fado em at\u00e9 3 chunks, retornando uma lista de dicion\u00e1rios.\n\n    A fun\u00e7\u00e3o usa o modelo de linguagem do `embedder` para extrair os metadados da s\u00famula e dividir o texto em at\u00e9 tr\u00eas partes principais:\n\n    1. conteudo_principal: O conte\u00fado principal do documento at\u00e9 a se\u00e7\u00e3o de \"REFER\u00caNCIAS NORMATIVAS\".\n\n    2. referencias_normativas: O conte\u00fado entre \"REFER\u00caNCIAS NORMATIVAS:\" e \"PRECEDENTES:\".\n\n    3. precedentes: O conte\u00fado ap\u00f3s a se\u00e7\u00e3o de \"PRECEDENTES:\".\n\n    Args:\n        file_path (str): Caminho para o arquivo PDF que ser\u00e1 processado.\n        embedder (EmbeddingSelfQuery): O objeto que cont\u00e9m o modelo de linguagem usado para extrair os metadados e chunks.\n\n    Returns:\n        processed (List[Dict[str, Any]]): Uma lista de dicion\u00e1rios contendo os textos dos chunks e seus respectivos metadados.\n            Cada dicion\u00e1rio tem a estrutura:\n            ```json\n            {\n                \"text\": &lt;texto do chunk&gt;,\n                \"metadata\": {\n                    \"num_sumula\": &lt;n\u00famero da s\u00famula&gt;,\n                    \"data_status\": &lt;data de status&gt;,\n                    \"data_status_ano\": &lt;ano da data de status&gt;,\n                    \"status_atual\": &lt;status atual&gt;,\n                    \"pdf_name\": &lt;nome do arquivo PDF&gt;,\n                    \"chunk_type\": &lt;tipo do chunk&gt;,\n                    \"chunk_index\": &lt;\u00edndice do chunk&gt;\n                }\n            }\n            ```\n    \"\"\"\n\n    pdf_name = os.path.basename(file_path)\n    result = md.convert(str(file_path))\n    text_content = result.text_content or \"\"\n\n    # Prompt de extra\u00e7\u00e3o\n    prompt = PROMPT_EXTRACT.format(\n        pdf_name=pdf_name, text_content=text_content[:12000]\n    )\n\n    try:\n        response = embedder.llm.invoke(prompt)\n        json_text = (\n            re.sub(r\"```[\\w-]*\", \"\", response.content)\n            .replace(\"```\", \"\")\n            .strip()\n        )\n        data = json.loads(json_text)\n\n        metadados = data.get(\"metadados\", {})\n        chunks = data.get(\"chunks\", {})\n\n        processed = []\n        for idx, (tipo, texto) in enumerate(chunks.items()):\n            if not texto or idx &gt;= 3:\n                continue\n            metadata = {\n                \"num_sumula\": metadados.get(\"num_sumula\"),\n                \"data_status\": metadados.get(\"data_status\"),\n                \"data_status_ano\": int(metadados.get(\"data_status_ano\")),\n                \"status_atual\": metadados.get(\"status_atual\"),\n                \"pdf_name\": metadados.get(\"pdf_name\", pdf_name),\n                \"chunk_type\": tipo,\n                \"chunk_index\": idx,\n            }\n            processed.append({\"text\": texto.strip(), \"metadata\": metadata})\n\n        return processed\n\n    except Exception as e:\n        logger.error(f\"\u26a0\ufe0f Erro ao processar {pdf_name}: {e}\")\n        return []\n</code></pre>"},{"location":"retriever/retriever/","title":"retriever","text":""},{"location":"retriever/retriever/#app.retrieval.retriever","title":"<code>app.retrieval.retriever</code>","text":""},{"location":"retriever/retriever/#app.retrieval.retriever.SelfQueryConfig","title":"<code>SelfQueryConfig</code>  <code>dataclass</code>","text":"<p>Configura\u00e7\u00e3o para o SelfQueryRetriever.</p> <p>Esta classe armazena as configura\u00e7\u00f5es usadas ao construir o retriever, como o nome da cole\u00e7\u00e3o no banco de dados Qdrant e o n\u00famero de resultados desejados.</p> <p>Attributes:</p> Name Type Description <code>collection_name</code> <code>str</code> <p>Nome da cole\u00e7\u00e3o do Qdrant (padr\u00e3o: \"sumulas_jornada\").</p> <code>k</code> <code>int</code> <p>N\u00famero de resultados a serem retornados na consulta (padr\u00e3o: 10).</p> Source code in <code>app/retrieval/retriever.py</code> <pre><code>@dataclass\nclass SelfQueryConfig:\n    \"\"\"\n    Configura\u00e7\u00e3o para o SelfQueryRetriever.\n\n    Esta classe armazena as configura\u00e7\u00f5es usadas ao construir o retriever, como o nome da cole\u00e7\u00e3o no banco de dados Qdrant e o n\u00famero de resultados desejados.\n\n    Attributes:\n        collection_name (str): Nome da cole\u00e7\u00e3o do Qdrant (padr\u00e3o: \"sumulas_jornada\").\n        k (int): N\u00famero de resultados a serem retornados na consulta (padr\u00e3o: 10).\n    \"\"\"\n\n    collection_name: str = \"sumulas_jornada\"\n    k: int = 10\n</code></pre>"},{"location":"retriever/retriever/#app.retrieval.retriever.build_self_query_retriever","title":"<code>build_self_query_retriever(cfg)</code>","text":"<p>Cria o SelfQueryRetriever sobre o QdrantVectorStore.</p> <p>Esta fun\u00e7\u00e3o configura e retorna um <code>SelfQueryRetriever</code> utilizando a configura\u00e7\u00e3o fornecida. O retriever \u00e9 configurado para realizar buscas usando a cole\u00e7\u00e3o do Qdrant e o modelo de LLM fornecido pelo embedder.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>SelfQueryConfig</code> <p>Configura\u00e7\u00e3o contendo o nome da cole\u00e7\u00e3o e o n\u00famero de resultados.</p> required <p>Returns:</p> Name Type Description <code>retriever</code> <code>SelfQueryRetriever</code> <p>Um objeto configurado para realizar consultas no banco de dados Qdrant com base na configura\u00e7\u00e3o fornecida.</p> Source code in <code>app/retrieval/retriever.py</code> <pre><code>def build_self_query_retriever(cfg: SelfQueryConfig) -&gt; SelfQueryRetriever:\n    \"\"\"\n    Cria o SelfQueryRetriever sobre o QdrantVectorStore.\n\n    Esta fun\u00e7\u00e3o configura e retorna um `SelfQueryRetriever` utilizando a configura\u00e7\u00e3o fornecida.\n    O retriever \u00e9 configurado para realizar buscas usando a cole\u00e7\u00e3o do Qdrant e o modelo de LLM fornecido pelo embedder.\n\n    Args:\n        cfg (SelfQueryConfig): Configura\u00e7\u00e3o contendo o nome da cole\u00e7\u00e3o e o n\u00famero de resultados.\n\n    Returns:\n        retriever (SelfQueryRetriever): Um objeto configurado para realizar consultas no banco de dados Qdrant com base na configura\u00e7\u00e3o fornecida.\n    \"\"\"\n    embedder = EmbeddingSelfQuery()\n    vectorstore = embedder.get_qdrant_vector_store(cfg.collection_name)\n\n    retriever = SelfQueryRetriever.from_llm(\n        llm=embedder.llm,\n        vectorstore=vectorstore,\n        document_contents=document_content_description,\n        metadata_field_info=metadata_field_info,\n        enable_limit=True,\n        search_kwargs={\"k\": cfg.k},\n    )\n    return retriever\n</code></pre>"},{"location":"retriever/retriever/#app.retrieval.retriever.search","title":"<code>search(query, cfg=None)</code>","text":"<p>Consulta usando self-query: o LLM infere termos SEM\u00c2NTICOS e tamb\u00e9m FILTROS de metadado.</p> <p>Esta fun\u00e7\u00e3o realiza uma consulta no banco de dados Qdrant usando um modelo de linguagem (LLM) para inferir termos sem\u00e2nticos e filtros de metadados. O resultado \u00e9 uma lista de documentos que correspondem \u00e0 consulta.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>A consulta textual a ser realizada.</p> required <code>cfg</code> <code>Optional[SelfQueryConfig]</code> <p>A configura\u00e7\u00e3o personalizada para o retriever. Se n\u00e3o fornecido, usa a configura\u00e7\u00e3o padr\u00e3o.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>Lista de documentos (<code>Document</code>) que correspondem \u00e0 consulta, incluindo metadados e conte\u00fado relevante.</p> Source code in <code>app/retrieval/retriever.py</code> <pre><code>def search(\n    query: str,\n    cfg: Optional[SelfQueryConfig] = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Consulta usando self-query: o LLM infere termos SEM\u00c2NTICOS e tamb\u00e9m FILTROS de metadado.\n\n    Esta fun\u00e7\u00e3o realiza uma consulta no banco de dados Qdrant usando um modelo de linguagem (LLM) para inferir termos sem\u00e2nticos\n    e filtros de metadados. O resultado \u00e9 uma lista de documentos que correspondem \u00e0 consulta.\n\n    Args:\n        query (str): A consulta textual a ser realizada.\n        cfg (Optional[SelfQueryConfig]): A configura\u00e7\u00e3o personalizada para o retriever. Se n\u00e3o fornecido, usa a configura\u00e7\u00e3o padr\u00e3o.\n\n    Returns:\n        (List[Document]): Lista de documentos (`Document`) que correspondem \u00e0 consulta, incluindo metadados e conte\u00fado relevante.\n    \"\"\"\n    cfg = cfg or SelfQueryConfig()\n    retriever = build_self_query_retriever(cfg)\n    # .invoke() retorna List[Document]\n    return retriever.invoke(query)\n</code></pre>"},{"location":"retriever/self_query/","title":"self_query","text":""},{"location":"retriever/self_query/#app.retrieval.self_query","title":"<code>app.retrieval.self_query</code>","text":""}]}